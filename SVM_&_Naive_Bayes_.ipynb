{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive Bayes\n",
        "\n",
        "\n",
        "Q1. What is Support Vector Machine (SVM)?\n",
        "- Support Vector Machine (SVM) is a powerful supervised machine learning algorithm that can be used for both classification and regression tasks, although it is most commonly applied in classification problems.\n",
        "\n",
        "  The main concept behind SVM is to find an optimal hyperplane that best separates the data points of different classes in the feature space. This separation is done in such a way that the margin (the distance between the hyperplane and the closest data points from each class, known as support vectors) is maximized.\n",
        "\n",
        "  If the data is not linearly separable in its original space, SVM uses kernel functions to project the data into a higher-dimensional space where a linear separation is possible.\n",
        "- How SVM Works\n",
        "  1. Linear Separation\n",
        "\n",
        " -  If the data is linearly separable, SVM finds a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions) that separates the classes with the maximum margin.\n",
        "\n",
        "  2. Maximizing the Margin\n",
        "\n",
        "  - A large margin reduces the chance of misclassification and improves generalization to new data.\n",
        "\n",
        "  3. Handling Non-linear Data\n",
        "\n",
        "- If the data is not linearly separable, SVM uses the kernel trick to transform the data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "-  Common kernels include:\n",
        "\n",
        " - Linear\n",
        "\n",
        " - Polynomial\n",
        "\n",
        " - Radial Basis Function (RBF)\n",
        "\n",
        " - Sigmoid\n",
        "\n",
        "  4. Soft Margin for Noisy Data\n",
        "\n",
        "-  In real-world data, perfect separation may not be possible. SVM introduces a soft margin controlled by a parameter C that allows some misclassifications in exchange for better generalization.\n",
        "\n",
        "Q2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "-  Difference Between Hard Margin and Soft Margin SVM\n",
        "  1. Hard Margin SVM\n",
        "\n",
        "-   Definition:\n",
        "  \n",
        "  A Hard Margin SVM tries to find a hyperplane that separates the data perfectly without any misclassification.\n",
        "\n",
        "- When Used:\n",
        "\n",
        " - Data is linearly separable (no overlap between classes).\n",
        "\n",
        " - No noise or outliers in the dataset.\n",
        "\n",
        "-  Advantages:\n",
        "\n",
        " - Clear and strict separation between classes.\n",
        "\n",
        " - Simpler decision boundary when perfect separation is possible.\n",
        "\n",
        "- Disadvantages:\n",
        "\n",
        " - Very sensitive to noise and outliers — even one misclassified point can make perfect separation impossible.\n",
        "\n",
        "- Example:\n",
        "\n",
        "  If we have two completely distinct groups of points with no overlap, Hard Margin SVM will create a boundary with zero tolerance for misclassification.\n",
        "\n",
        "2. Soft Margin SVM\n",
        "\n",
        "- Definition:\n",
        "  \n",
        "  A Soft Margin SVM allows some misclassification or overlap between classes to achieve a better generalization on unseen data. This is controlled by a regularization parameter C.\n",
        "\n",
        "-  When Used:\n",
        "\n",
        " - Data is not perfectly separable.\n",
        "\n",
        " - There may be noise or overlapping class distributions.\n",
        "\n",
        "-  Advantages:\n",
        "\n",
        " - More robust to noise and outliers.\n",
        "\n",
        " - Works with real-world datasets where perfect separation is rare.\n",
        "\n",
        "- Disadvantages:\n",
        "\n",
        " - May misclassify some training points to improve generalization.\n",
        "\n",
        "- Example:\n",
        " - In a spam email classification problem, some borderline emails might be misclassified to ensure the model performs better on new, unseen emails.\n",
        "\n",
        "\n",
        "\n",
        "Q3.  What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "-  Kernel Trick in SVM\n",
        "\n",
        "  Definition:\n",
        "\n",
        "  The Kernel Trick is a mathematical technique used in Support Vector Machines (SVM) to enable the algorithm to separate data that is not linearly separable in its original space.\n",
        "  \n",
        "   It works by implicitly mapping the input data into a higher-dimensional feature space without explicitly computing the transformation. In this higher-dimensional space, the data can often be separated by a linear hyperplane.\n",
        "\n",
        "   The key advantage is that the kernel trick allows complex, non-linear boundaries to be learned without high computational cost of directly transforming the data.\n",
        "\n",
        "-  How It Works\n",
        "  \n",
        " 1. Instead of computing a transformation φ(x) explicitly, the kernel trick computes the dot product in the higher-dimensional space using a kernel function K(xᵢ, xⱼ) directly in the original space.\n",
        "\n",
        "  2. This avoids handling high-dimensional vectors explicitly and makes the computation more efficient.\n",
        "\n",
        "  Mathematically:\n",
        "\n",
        "   K(xᵢ, xⱼ) = φ(xᵢ) · φ(xⱼ)\n",
        "\n",
        "- Example Kernel: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "  Formula:\n",
        "  \n",
        "  K(x, x′) = exp(−γ ||x − x′||²)\n",
        "- Where:\n",
        "\n",
        " - γ (gamma) controls how far the influence of a single training example reaches.\n",
        "\n",
        "-  Use Case:\n",
        "\n",
        " - RBF kernel is widely used when the decision boundary between classes is non-linear and complex.\n",
        "\n",
        " - For example, in handwriting recognition (digits 0–9), the RBF kernel can separate similar-looking digits like “4” and “9” by mapping their pixel data into a higher dimension where separation is easier."
      ],
      "metadata": {
        "id": "nL25Abdmnpa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.   What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "  i. Naïve Bayes Classifier\n",
        "\n",
        "  - Definition:\n",
        "\n",
        "   The Naïve Bayes Classifier is a family of probabilistic machine learning algorithms based on Bayes’ Theorem.\n",
        "  \n",
        "  It is primarily used for classification tasks, especially in text classification (e.g., spam detection, sentiment analysis).\n",
        "\n",
        " - Bayes’ Theorem:\n",
        "\n",
        "     P(A|B) = [ P(B|A) × P(A) ] / P(B)\n",
        "\n",
        "    Where:\n",
        "\n",
        " - P(A|B): Posterior probability — probability of class A given evidence B.\n",
        "\n",
        " - P(B|A): Likelihood — probability of evidence B given class A.\n",
        "\n",
        " - P(A): Prior probability of class A.\n",
        "\n",
        " - P(B): Prior probability of evidence B.\n",
        "\n",
        "ii.   Why is it Called “Naïve”?\n",
        "\n",
        "   - It is called “naïve” because it assumes that all features (predictors) are independent of each other given the class label.\n",
        "In real-world datasets, this assumption is often not true — features can be correlated — but the classifier still works surprisingly well in many situations despite this unrealistic assumption.\n",
        "\n",
        "Q5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "   1. Gaussian Naïve Bayes\n",
        "  \n",
        "  Definition:\n",
        "\n",
        " - Assumes that the continuous features follow a Gaussian (Normal) distribution within each class.\n",
        "\n",
        " - Probability density function is calculated using the Gaussian formula.\n",
        "\n",
        " Formula:\n",
        "      \n",
        "           P(xᵢ | y) = (1 / √(2πσ²)) × exp(−(xᵢ − μ)² / (2σ²))\n",
        "\n",
        "  Where μ is the mean and σ² is the variance of the feature values for class y.\n",
        "\n",
        " When to Use:\n",
        "\n",
        " - When the features are continuous and can be modeled with a bell-shaped curve.\n",
        "\n",
        " - Example: Predicting a person’s health risk based on continuous measurements like blood pressure, height, or weight.\n",
        "\n",
        "  2. Multinomial Naïve Bayes\n",
        "\n",
        " Definition:\n",
        "\n",
        " - Designed for discrete features, particularly count data.\n",
        "\n",
        " - Often used for document classification where features represent the frequency of terms in a document.\n",
        "\n",
        " When to Use:\n",
        "\n",
        " - When features represent counts or frequencies.\n",
        "\n",
        " - Example: Spam email detection using the number of times a specific word appears in an email.\n",
        "\n",
        "  3. Bernoulli Naïve Bayes\n",
        "\n",
        "   Definition:\n",
        "\n",
        " - Assumes that features are binary (0 or 1), indicating the presence or absence of a particular feature.\n",
        "\n",
        " - Unlike Multinomial NB, it only considers whether a feature is present, not how many times it appears.\n",
        "\n",
        " When to Use:\n",
        "\n",
        " - When features are binary indicators.\n",
        "\n",
        " - Example: Classifying documents based on whether certain keywords appear or not (regardless of frequency).\n",
        "\n",
        "| Variant     | Data Type  | Example                            |\n",
        "| ----------- | ---------- | ---------------------------------- |\n",
        "| Gaussian    | Continuous | Predicting disease from lab values |\n",
        "| Multinomial | Count data | Spam filtering                     |\n",
        "| Bernoulli   | Binary     | Keyword presence classification    |\n",
        "\n",
        "\n",
        "- Gaussian Naïve Bayes: Continuous data (e.g., Iris, Breast Cancer dataset).\n",
        "\n",
        "- Multinomial Naïve Bayes: Discrete count data (e.g., word frequencies).\n",
        "\n",
        "- Bernoulli Naïve Bayes: Binary features (e.g., keyword presence).\n"
      ],
      "metadata": {
        "id": "xCdNQM0aqskD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset (Iris)\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 1. Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "print(\"Gaussian NB Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
        "\n",
        "# 2. Multinomial Naïve Bayes (need non-negative integer features)\n",
        "# For demo, converting to integer counts\n",
        "import numpy as np\n",
        "X_train_counts = np.abs(X_train.astype(int))\n",
        "X_test_counts = np.abs(X_test.astype(int))\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_counts, y_train)\n",
        "y_pred_mnb = mnb.predict(X_test_counts)\n",
        "print(\"Multinomial NB Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
        "\n",
        "# 3. Bernoulli Naïve Bayes (binary features)\n",
        "X_train_bin = (X_train_counts > 0).astype(int)\n",
        "X_test_bin = (X_test_counts > 0).astype(int)\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train_bin, y_train)\n",
        "y_pred_bnb = bnb.predict(X_test_bin)\n",
        "print(\"Bernoulli NB Accuracy:\", accuracy_score(y_test, y_pred_bnb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdueored31Wt",
        "outputId": "a95fede0-d518-4781-d6b7-c1b7f824206f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian NB Accuracy: 0.9111111111111111\n",
            "Multinomial NB Accuracy: 0.8222222222222222\n",
            "Bernoulli NB Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "cZB_2vMa36qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train SVM classifier with a linear kernel\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print support vectors\n",
        "print(\"\\nSupport Vectors:\\n\", svm_clf.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BUeTLCT4mtN",
        "outputId": "ca87fcac-d10f-46ee-fd92-5699324e8bf3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.  2.2 5.  1.5]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [7.2 3.  5.8 1.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "BgDwmr7d4p1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4jNxnF748bl",
        "outputId": "8c21ddf6-fd86-404b-bbf0-189320c9b236"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.97      0.89      0.93        64\n",
            "      benign       0.94      0.98      0.96       107\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.95      0.94      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.  Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "UQTRN3qD5xQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create SVM model\n",
        "svm_clf = SVC()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(svm_clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best model prediction\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNH2nF-26G7T",
        "outputId": "189f10ef-27d7-4a22-9075-1790b5aad4f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.  Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Xe6dd8IR6PGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load dataset (two categories for binary ROC-AUC)\n",
        "categories = ['rec.sport.hockey', 'sci.space']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Train Naive Bayes Classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 5. Predict probabilities\n",
        "y_prob = clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# 6. Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", round(roc_auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhJ51xk86Wuq",
        "outputId": "b103affe-2cb8-4b22-e72a-7bd9e5925d5d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.  Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)\n",
        "  1. Approach\n",
        "  a. Preprocessing\n",
        "\n",
        "- Handling Missing Data:\n",
        "\n",
        " - Remove completely empty emails or replace missing text with an empty string (\"\").\n",
        "\n",
        " - Ensure there are no NaN values before vectorization.\n",
        "\n",
        "- Text Vectorization:\n",
        "\n",
        " - Use TF-IDF Vectorization (TfidfVectorizer) to convert text into numerical features.\n",
        "\n",
        " - Lowercasing, removing stopwords, and possibly stemming/lemmatization can help.\n",
        "\n",
        "  b. Model Choice\n",
        "- Naïve Bayes (MultinomialNB) works well for text classification, especially when word frequency is important and vocabulary is large. It is fast and interpretable.\n",
        "\n",
        "- SVM (LinearSVC) can handle high-dimensional sparse data well and often yields slightly better accuracy, but is slower and doesn't produce direct probability estimates.\n",
        "\n",
        "- Choice: Start with MultinomialNB for speed and good baseline performance. Move to SVM if higher precision is needed and computational cost is acceptable.\n",
        "\n",
        "  c. Addressing Class Imbalance\n",
        "- Spam datasets often have more \"Not Spam\" than \"Spam\".\n",
        "\n",
        "- Solutions:\n",
        "\n",
        " - Use class_weight='balanced' (for SVM) or adjust prior probabilities (for Naïve Bayes).\n",
        "\n",
        " - Apply oversampling (SMOTE) or undersampling.\n",
        "\n",
        " - Use precision-recall metrics instead of accuracy.\n",
        "\n",
        "  d. Evaluation Metrics\n",
        "- Since we care more about catching spam without flagging too many legit emails, use:\n",
        "\n",
        " - Precision: % of predicted spam that is actually spam.\n",
        "\n",
        " - Recall: % of actual spam correctly detected.\n",
        "\n",
        " - F1-score: Balance of precision & recall.\n",
        "\n",
        " - ROC-AUC: Overall separability.\n",
        "\n",
        "  e. Business Impact\n",
        "- Positive Impact:\n",
        "\n",
        " - Reduce spam reaching customers, improving trust.\n",
        "\n",
        " - Save employees' time by reducing inbox clutter.\n",
        "\n",
        " - Lower phishing risks → protect company data.\n",
        "\n",
        "- Risks:\n",
        "\n",
        " - Too many false positives → important emails lost.\n",
        "\n",
        " - Balance threshold to minimize business disruption.\n",
        "\n",
        " 2. Python Implementation"
      ],
      "metadata": {
        "id": "8RwSu-eA7D_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Example synthetic dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        'Win money now!!!', 'Lowest price on meds', 'Meeting at 10am', 'Lunch plans today?',\n",
        "        'You have won a lottery', 'Your invoice is attached', 'Buy cheap products now',\n",
        "        'Let’s catch up tomorrow', None, 'Congratulations, you won!'\n",
        "    ],\n",
        "    'label': [1, 1, 0, 0, 1, 0, 1, 0, 0, 1]  # 1 = Spam, 0 = Not Spam\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handle missing values\n",
        "df['text'] = df['text'].fillna(\"\")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.3, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "y_prob = clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", round(roc_auc_score(y_test, y_prob), 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Beg2YDyF-QZp",
        "outputId": "772ba378-9b17-41c3-ce4d-82a96c2354e4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.33      1.00      0.50         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.17      0.50      0.25         3\n",
            "weighted avg       0.11      0.33      0.17         3\n",
            "\n",
            "ROC-AUC Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}